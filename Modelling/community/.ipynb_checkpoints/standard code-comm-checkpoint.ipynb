{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(file):\n",
    "    data = pd.read_csv(file)\n",
    "    print('Raw shape: ',data.shape)\n",
    "#     data['Date'] = pd.to_datetime(data.Date)\n",
    "    print('Days: ',len(set(data.Date)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTimeSeries(df,freq):\n",
    "    table = pd.pivot_table(df, values='vehicle_count', index=['Date','Hour','Min'],\n",
    "                    columns=['DOLocationID'], aggfunc=np.sum, fill_value=0)\n",
    "    time = pd.date_range('2018-01-01', '2018-12-31',freq=freq)\n",
    "    time = pd.DataFrame(time, columns=['time'])\n",
    "    \n",
    "    time['Date'] = time['time'].dt.date.astype('str')\n",
    "    time['Hour'] = time['time'].dt.hour.astype('int')\n",
    "    time['Min'] = time['time'].dt.minute.astype('int')\n",
    "    del time['time']\n",
    "\n",
    "    table = table.merge(time, on=['Date','Hour','Min'], how='right')\n",
    "    table.fillna(0, inplace=True)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscoreNormalizeSpatial(matrix):\n",
    "    m = matrix.copy()\n",
    "    for i in range(m.shape[0]):\n",
    "        m[i, :] = (m[i, :] - m[i, :].mean()) / (m[i, :].std()+1e-10)\n",
    "        \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(matrix):\n",
    "    m = matrix.copy()\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(m)\n",
    "    t = scaler.transform(m)\n",
    "    return scaler, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addLag(dataset, maxlag, lagColumns):\n",
    "    dataset_list = [dataset]\n",
    "\n",
    "    for l in range(1, maxlag+1):\n",
    "        df = dataset.shift(l)\n",
    "        df = df[lagColumns]\n",
    "        df.columns = [c+'_lag_'+str(l) for c in df.columns]\n",
    "        dataset_list.append(df)\n",
    "\n",
    "    dataset = pd.concat(dataset_list, axis=1)\n",
    "    dataset = dataset.iloc[maxlag:]\n",
    "    dataset = dataset.fillna(0)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Community Level Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def communityAggregation(rawdata,community=6):\n",
    "    filePath = rootDir + 'Data/ZonetoComm.csv'\n",
    "    zones = pd.read_csv(filePath)\n",
    "    if community == 6:\n",
    "        zones['start_community'] = zones.start_community.astype(int)\n",
    "    elif community == 24:\n",
    "        zones['start_community'] = zones.start_community.astype(str)\n",
    "    zontoComm = dict(zip(zones.start_id.values,zones.start_community.values))\n",
    "    agg_data = rawdata.copy(deep=True)\n",
    "    agg_data['DOLocationID'] = agg_data['DOLocationID'].apply(lambda x:zontoComm[x])\n",
    "    agg_data = getTimeSeries(agg_data)\n",
    "    agg_data = agg_data.reset_index()\n",
    "    zone_weights = get_weights(rawdata, zontoComm)\n",
    "    targetColumns = sorted(zones['start_community'].unique().tolist())\n",
    "    lagColumns = targetColumns + ['arrival']\n",
    "    DateColumns = ['Date']\n",
    "    return (agg_data, zone_weights,lagColumns,targetColumns,DateColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(rawdata, zontoBorough):\n",
    "    \n",
    "    rawdata['Borough'] = rawdata['DOLocationID'].apply(lambda x:zontoBorough[x])\n",
    "    \n",
    "    borough_df = rawdata[['vehicle_count','Borough']].groupby(by='Borough').sum().reset_index()\n",
    "\n",
    "    zone_df = rawdata[['vehicle_count','DOLocationID']].groupby(by='DOLocationID').sum().reset_index()\n",
    "\n",
    "    zone_df['Borough'] = zone_df['DOLocationID'].apply(lambda x:zontoBorough[x])\n",
    "\n",
    "    zone_df = pd.merge(borough_df, zone_df, on=['Borough'], how='inner')\n",
    "\n",
    "    zone_df['zone_weight'] = zone_df.vehicle_count_y / zone_df.vehicle_count_x\n",
    "\n",
    "    zone_df = zone_df[['Borough', 'DOLocationID', 'zone_weight']]\n",
    "\n",
    "    return zone_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge External Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def externalFeatures(hub,agg_data, maxlag, lagColumns):\n",
    "    externalDataDir = rootDir+'HongData/'\n",
    "    extFile = externalDataDir + hub.upper() + \".csv\"\n",
    "    extDf = pd.read_csv(extFile)\n",
    "    extDf['date'] = pd.to_datetime(extDf['date'], yearfirst=True)\n",
    "    extDf['Hour'] = extDf['date'].dt.hour\n",
    "    extDf['Dow'] = extDf['date'].dt.dayofweek\n",
    "    extDf['Date'] = extDf['date'].dt.date\n",
    "    extDf['Min'] = extDf['date'].dt.minute\n",
    "    selected_columns = ['Date', 'Hour', 'Dow', 'arrival','maxtemp', 'mintemp', 'avgtemp', 'departure', 'hdd',\n",
    "       'cdd', 'participation', 'newsnow', 'snowdepth', 'ifSnow']\n",
    "    extDf = extDf[selected_columns]\n",
    "    agg_data['Date'] = pd.to_datetime(agg_data['Date'])\n",
    "    extDf['Date'] = pd.to_datetime(extDf['Date'])\n",
    "    agg_data = pd.merge(agg_data,extDf, on=['Date', 'Hour'], how='inner')\n",
    "    agg_data['Date'] = agg_data['Date'].dt.date\n",
    "    agg_data_lag = addLag(agg_data, maxlag, lagColumns)\n",
    "    return agg_data_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub = 'LGA'\n",
    "granularity = 15\n",
    "tune_hyp_params = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootDir = '/home/mingyi/Dropbox/UrbanTemporalNetworks/'\n",
    "dataDir = rootDir + 'processedData/'\n",
    "\n",
    "file = dataDir + hub + 'VehicleBy'+str(granularity)+'Min.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw shape:  (2201234, 5)\n",
      "Days:  365\n"
     ]
    }
   ],
   "source": [
    "rawdata = loadData(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Min</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2018-01-01</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2018-12-30</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">23</th>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34945 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1    2    3    4    5    6    7    8    9    10   ...  \\\n",
       "Date       Hour Min                                                    ...   \n",
       "2018-01-01 0    0    0.0  0.0  0.0  0.0  0.0  0.0  5.0  0.0  1.0  0.0  ...   \n",
       "                15   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "                30   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "                45   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   \n",
       "           1    0    0.0  0.0  0.0  0.0  0.0  0.0  3.0  0.0  0.0  0.0  ...   \n",
       "...                  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "2018-12-30 23   0    0.0  0.0  0.0  2.0  0.0  0.0  6.0  0.0  0.0  0.0  ...   \n",
       "                15   0.0  0.0  0.0  6.0  0.0  0.0  5.0  0.0  0.0  0.0  ...   \n",
       "                30   0.0  0.0  0.0  1.0  0.0  0.0  6.0  0.0  0.0  1.0  ...   \n",
       "                45   0.0  0.0  2.0  0.0  0.0  0.0  9.0  0.0  1.0  0.0  ...   \n",
       "2018-12-31 0    0    0.0  0.0  1.0  1.0  0.0  0.0  8.0  0.0  0.0  1.0  ...   \n",
       "\n",
       "                     254   255  256  257  258  259  260  261  262   263  \n",
       "Date       Hour Min                                                      \n",
       "2018-01-01 0    0    0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  3.0   2.0  \n",
       "                15   0.0   0.0  1.0  0.0  0.0  0.0  1.0  0.0  0.0   0.0  \n",
       "                30   0.0   0.0  0.0  0.0  0.0  2.0  0.0  0.0  0.0   0.0  \n",
       "                45   0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0   0.0  \n",
       "           1    0    0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   1.0  \n",
       "...                  ...   ...  ...  ...  ...  ...  ...  ...  ...   ...  \n",
       "2018-12-30 23   0    0.0   1.0  5.0  0.0  1.0  2.0  1.0  0.0  2.0   4.0  \n",
       "                15   1.0  11.0  1.0  0.0  1.0  0.0  4.0  6.0  8.0  12.0  \n",
       "                30   0.0   2.0  8.0  1.0  0.0  1.0  0.0  0.0  1.0   9.0  \n",
       "                45   0.0   6.0  1.0  1.0  0.0  0.0  2.0  1.0  3.0   0.0  \n",
       "2018-12-31 0    0    0.0   2.0  2.0  0.0  1.0  0.0  0.0  1.0  1.0   7.0  \n",
       "\n",
       "[34945 rows x 258 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = str(granularity)+'min'\n",
    "edge_data = getTimeSeries(rawdata,freq=freq)\n",
    "edge_data = edge_data.set_index(['Date','Hour','Min'])\n",
    "edge_data.sort_values(by=['Date','Hour','Min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = str(granularity)+'min'\n",
    "comm = 24\n",
    "maxlag = 24\n",
    "zones = pd.read_csv('/home/mingyi/Dropbox/UrbanTemporalNetworks/Data/ZonetoComm.csv')\n",
    "if comm == 6:\n",
    "    zones['start_community'] = zones.start_community.astype(int)\n",
    "elif comm == 24:\n",
    "    zones['start_community'] = zones.start_community.astype(str)\n",
    "zontoComm = dict(zip(zones.start_id.values,zones.start_community.values))\n",
    "\n",
    "comm_data = rawdata.copy(deep=True)\n",
    "comm_data['DOLocationID'] = comm_data['DOLocationID'].apply(lambda x:zontoComm[x])\n",
    "\n",
    "comm_data = getTimeSeries(comm_data, '15min')\n",
    "comm_data = comm_data.reset_index()\n",
    "zone_weights = get_weights(rawdata, zontoComm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = str(granularity)+'min'\n",
    "comm = 24\n",
    "maxlag = 24\n",
    "zones = pd.read_csv('/home/mingyi/Dropbox/UrbanTemporalNetworks/Data/ZonetoComm.csv')\n",
    "if comm == 6:\n",
    "    zones['start_community'] = zones.start_community.astype(int)\n",
    "elif comm == 24:\n",
    "    zones['start_community'] = zones.start_community.astype(str)\n",
    "zontoComm = dict(zip(zones.start_id.values,zones.start_community.values))\n",
    "\n",
    "comm_data = rawdata.copy(deep=True)\n",
    "comm_data['DOLocationID'] = comm_data['DOLocationID'].apply(lambda x:zontoComm[x])\n",
    "\n",
    "comm_data = getTimeSeries(comm_data, '15min')\n",
    "comm_data = comm_data.reset_index()\n",
    "zone_weights = get_weights(rawdata, zontoComm)\n",
    "\n",
    "externalDataDir = \"/HongData/\"+str(granularity)+'min/'\n",
    "extFile = rootDir + externalDataDir + hub.upper() + \".csv\"\n",
    "extDf = pd.read_csv(extFile)\n",
    "# extDf['date'] = pd.to_datetime(extDf['date'], yearfirst=True)\n",
    "extDf['Hour'] = extDf['Hour'].astype('int')\n",
    "extDf['Min'] = extDf['Min'].astype('int')\n",
    "\n",
    "selected_columns = ['Date', 'Hour','Min', 'Dow', 'arrival','maxtemp', 'mintemp', 'avgtemp', 'departure', 'hdd',\n",
    "       'cdd', 'participation', 'newsnow', 'snowdepth', 'ifSnow']\n",
    "\n",
    "\n",
    "\n",
    "extDf = extDf[selected_columns]\n",
    "\n",
    "comm_data = pd.merge(comm_data,extDf, on=['Date', 'Hour','Min'], how='inner')\n",
    "comm_data = comm_data.sort_values(by=['Date','Hour','Min'])\n",
    "if comm == 6:\n",
    "    lagColumns = ['0','1', '2', '3', '4', '5','arrival']\n",
    "else:\n",
    "    lagColumns = ['0.0', '0.1', '0.2', '1.0', '1.1', '1.2', '1.3', '2.0',\n",
    "       '2.1', '2.2', '2.3', '3.0', '3.1', '3.2', '4.0', '4.1', '4.2', '4.3',\n",
    "       '4.4', '4.5', '5.0', '5.1', '5.2', '5.3','arrival']\n",
    "targetColumns = [col for col in lagColumns if col!='arrival']\n",
    "DateColumns = ['Date','Hour','Min']\n",
    "comm_data_lag = addLag(comm_data, maxlag, lagColumns)\n",
    "comm_data_lag = comm_data_lag.sort_values(by=['Date','Hour','Min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "maxlag = 12\n",
    "agg_data, zone_weights, lagColumns,targetColumns,DateColumns = communityAggregation(rawdata,community=24)\n",
    "comm_data_lag = externalFeatures(hub,agg_data, maxlag, lagColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "month:  1\n",
      "Train Size:  (31969, 640)\n",
      "Test Size:  (2952, 640)\n",
      "edge test data shape:  (2952, 258)\n"
     ]
    }
   ],
   "source": [
    "CommR2List = []\n",
    "EdgeR2List = []\n",
    "# residualDf_list = []\n",
    "networkPrediction = pd.DataFrame()\n",
    "\n",
    "for m in range(1,13):\n",
    "    print()\n",
    "    print(\"month: \",m)\n",
    "    month_index  = pd.to_datetime(comm_data_lag.Date).dt.month == m\n",
    "\n",
    "    dataset_train = comm_data_lag[~month_index]\n",
    "    dataset_test = comm_data_lag[month_index]\n",
    "    print(\"Train Size: \",dataset_train.shape)\n",
    "    print(\"Test Size: \",dataset_test.shape)\n",
    "\n",
    "    edgeMonthIndex = [False] * maxlag + list(month_index)\n",
    "    edge_testData = edge_data[edgeMonthIndex]\n",
    "    select_cols = [c for c in edge_testData.columns if c not in ['Date','Hour']]\n",
    "    edge_testData = edge_testData[select_cols]\n",
    "    print(\"edge test data shape: \",edge_testData.shape)\n",
    "\n",
    "\n",
    "    X_train = dataset_train.drop(targetColumns+DateColumns , axis = 1)\n",
    "    X_test = dataset_test.drop(targetColumns+DateColumns , axis = 1)\n",
    "    y_train = dataset_train[targetColumns]\n",
    "    y_test = dataset_test[targetColumns]\n",
    "\n",
    "    rf2 = RandomForestRegressor(random_state = 2019, n_estimators=150, \n",
    "                               min_samples_split=3,\n",
    "                               min_samples_leaf= 2, \n",
    "                               max_features= 'sqrt',\n",
    "                               max_depth= None, \n",
    "                               bootstrap= False)\n",
    "\n",
    "    rf2.fit(X_train,y_train)\n",
    "\n",
    "    print(\"Train R2: \",rf2.score(X_train,y_train))\n",
    "    test_r2 = rf2.score(X_test,y_test)\n",
    "    print(\"Test R2: \",test_r2)\n",
    "\n",
    "\n",
    "    comm_prediction = rf2.predict(X_test)\n",
    "    edge_prediction_df = pd.DataFrame(comm_prediction)\n",
    "    edge_prediction_df.columns = y_test.columns\n",
    "\n",
    "    residual = y_test - comm_prediction\n",
    "    residual_df = dataset_test[['Date','Hour']]\n",
    "    residual_df = pd.concat([residual_df,pd.DataFrame(residual)], axis =1)\n",
    "\n",
    "    boroughs = list(edge_prediction_df.columns)\n",
    "    for bor in boroughs:\n",
    "    #     print(bor)\n",
    "\n",
    "        weight_df = zone_weights[zone_weights.Borough == bor]\n",
    "\n",
    "    #     print(len(weight_df.DOLocationID))\n",
    "\n",
    "        for b_zone,z_weight in zip(weight_df.DOLocationID.values,weight_df.zone_weight.values):        \n",
    "            edge_prediction_df[b_zone] = edge_prediction_df[bor] * z_weight\n",
    "\n",
    "\n",
    "    select_cols = [c for c in edge_prediction_df.columns if c not in boroughs]\n",
    "    edge_prediction_df = edge_prediction_df[select_cols]\n",
    "\n",
    "\n",
    "    edge_prediction_df = edge_prediction_df[edge_testData.columns]\n",
    "    \n",
    "    networkPrediction = pd.concat([networkPrediction,edge_prediction_df])\n",
    "    edge_r2 = r2_score(edge_testData.values, edge_prediction_df.values, multioutput='variance_weighted')\n",
    "    print(\"Edge R2: \",edge_r2)\n",
    "\n",
    "    CommR2List.append(test_r2)\n",
    "    EdgeR2List.append(edge_r2)\n",
    "    residualDf_list.append(residual_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "month:  1\n",
      "Train Size:  (15985, 639)\n",
      "Test Size:  (1464, 639)\n",
      "\n",
      "month:  2\n",
      "Train Size:  (16105, 639)\n",
      "Test Size:  (1344, 639)\n",
      "\n",
      "month:  3\n",
      "Train Size:  (15961, 639)\n",
      "Test Size:  (1488, 639)\n",
      "\n",
      "month:  4\n",
      "Train Size:  (16009, 639)\n",
      "Test Size:  (1440, 639)\n",
      "\n",
      "month:  5\n",
      "Train Size:  (15961, 639)\n",
      "Test Size:  (1488, 639)\n",
      "\n",
      "month:  6\n",
      "Train Size:  (16009, 639)\n",
      "Test Size:  (1440, 639)\n",
      "\n",
      "month:  7\n",
      "Train Size:  (15961, 639)\n",
      "Test Size:  (1488, 639)\n",
      "\n",
      "month:  8\n",
      "Train Size:  (15961, 639)\n",
      "Test Size:  (1488, 639)\n",
      "\n",
      "month:  9\n",
      "Train Size:  (16009, 639)\n",
      "Test Size:  (1440, 639)\n",
      "\n",
      "month:  10\n",
      "Train Size:  (15961, 639)\n",
      "Test Size:  (1488, 639)\n",
      "\n",
      "month:  11\n",
      "Train Size:  (16009, 639)\n",
      "Test Size:  (1440, 639)\n",
      "\n",
      "month:  12\n",
      "Train Size:  (16008, 639)\n",
      "Test Size:  (1441, 639)\n"
     ]
    }
   ],
   "source": [
    "networkPrediction = pd.DataFrame()\n",
    "networkPredictionStd = pd.DataFrame()\n",
    "PCAPredictedDF = pd.DataFrame()\n",
    "for m in range(1,13):\n",
    "    print()\n",
    "\n",
    "    print(\"month: \",m)\n",
    "    month_index  = pd.to_datetime(pcaData_lag.Date).dt.month == m\n",
    "\n",
    "    dataset_train = pcaData_lag[~month_index]\n",
    "    dataset_test = pcaData_lag[month_index]\n",
    "    print(\"Train Size: \",dataset_train.shape)\n",
    "    print(\"Test Size: \",dataset_test.shape)\n",
    "\n",
    "\n",
    "    X_train = dataset_train.drop(targetColumns+DateColumns , axis = 1)\n",
    "    X_test = dataset_test.drop(targetColumns+DateColumns , axis = 1)\n",
    "    y_train = dataset_train[targetColumns]\n",
    "    y_test = dataset_test[targetColumns]\n",
    "\n",
    "\n",
    "    rf2 = RandomForestRegressor(random_state = 2019, n_estimators=150, \n",
    "                               min_samples_split=3,\n",
    "                               min_samples_leaf= 2, \n",
    "                               max_features= 'sqrt',\n",
    "                               max_depth= None, \n",
    "                               bootstrap= False)\n",
    "\n",
    "    rf2.fit(X_train,y_train)\n",
    "    \n",
    "    PCAPredicted = rf2.predict(X_test)\n",
    "    PCAPredictedDF = pd.concat([PCAPredictedDF,pd.DataFrame(PCAPredicted)])\n",
    "    for no_tree in range(rf2.n_estimators):\n",
    "        predict = rf2.estimators_[no_tree].predict(X_test)\n",
    "        if no_tree == 0:\n",
    "            predict_values = predict\n",
    "        else:\n",
    "            predict_values = np.vstack((predict_values,predict))\n",
    "\n",
    "    predict_values_inversePCA = inverse_pca(predict_values,pca)\n",
    "    predict_values_inverseStandard = inverse_standardize(predict_values_inversePCA, scaler)\n",
    "    predict_values_inverseStandard_df = pd.DataFrame(predict_values_inverseStandard)\n",
    "    predict_values_inverseStandard_df['Date'] = np.tile(dataset_test['Date'],rf2.n_estimators)\n",
    "    predict_values_inverseStandard_df['Hour'] = np.tile(dataset_test['Hour'],rf2.n_estimators)\n",
    "    predict_values_inverseStandard_df['Min'] = np.tile(dataset_test['Min'],rf2.n_estimators)\n",
    "#     network_prediction_mean = predict_values_inverseStandard.mean(axis=1)\n",
    "#     network_prediction_std = predict_values_inverseStandard.std(axis=1)\n",
    "    network_prediction_mean_df = predict_values_inverseStandard_df.groupby(['Date','Hour','Min']).mean()\n",
    "    network_prediction_std_df = predict_values_inverseStandard_df.groupby(['Date','Hour','Min']).std()\n",
    "    \n",
    "    network_prediction_mean_df.columns = [str(col) + '_mean' for col in edge_data.columns]\n",
    "    networkPrediction = pd.concat([networkPrediction,network_prediction_mean_df])\n",
    "    \n",
    "    network_prediction_std_df.columns = [str(col) + '_std' for col in edge_data.columns]\n",
    "    networkPredictionStd = pd.concat([networkPredictionStd,network_prediction_std_df])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6521540440924711"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aggregated R2\n",
    "r2_score(pcaData_lag[targetColumns], PCAPredictedDF, multioutput='variance_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkPrediction.to_csv(rootDir+'/prediction/%sPCA'%hub+str(pca_comps)+'Mean'+\n",
    "                         str(granularity)+'Min'+str(maxlag)+'lag.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkPredictionStd.to_csv(rootDir+'/prediction/%sPCA'%hub+str(pca_comps)+'Std'+\n",
    "                            str(granularity)+'Min'+str(maxlag)+'lag.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.673448837970505"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(edge_data.iloc[maxlag:], networkPrediction, multioutput='variance_weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0_mean</th>\n",
       "      <th>1_mean</th>\n",
       "      <th>2_mean</th>\n",
       "      <th>3_mean</th>\n",
       "      <th>4_mean</th>\n",
       "      <th>5_mean</th>\n",
       "      <th>6_mean</th>\n",
       "      <th>7_mean</th>\n",
       "      <th>8_mean</th>\n",
       "      <th>9_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>256_mean</th>\n",
       "      <th>257_mean</th>\n",
       "      <th>258_mean</th>\n",
       "      <th>259_mean</th>\n",
       "      <th>260_mean</th>\n",
       "      <th>261_mean</th>\n",
       "      <th>262_mean</th>\n",
       "      <th>263_mean</th>\n",
       "      <th>264_mean</th>\n",
       "      <th>265_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>Min</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2018-01-01</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">6</th>\n",
       "      <th>0</th>\n",
       "      <td>0.017662</td>\n",
       "      <td>0.251026</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>0.101448</td>\n",
       "      <td>0.586587</td>\n",
       "      <td>0.019011</td>\n",
       "      <td>0.016041</td>\n",
       "      <td>1.587028</td>\n",
       "      <td>0.009788</td>\n",
       "      <td>0.202507</td>\n",
       "      <td>...</td>\n",
       "      <td>1.355020</td>\n",
       "      <td>0.423774</td>\n",
       "      <td>0.507407</td>\n",
       "      <td>0.140172</td>\n",
       "      <td>0.436238</td>\n",
       "      <td>0.837516</td>\n",
       "      <td>1.493704</td>\n",
       "      <td>1.971721</td>\n",
       "      <td>0.229733</td>\n",
       "      <td>21.341498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.017839</td>\n",
       "      <td>0.262316</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>0.096212</td>\n",
       "      <td>0.578203</td>\n",
       "      <td>0.018456</td>\n",
       "      <td>0.017548</td>\n",
       "      <td>1.548158</td>\n",
       "      <td>0.009548</td>\n",
       "      <td>0.193304</td>\n",
       "      <td>...</td>\n",
       "      <td>1.340479</td>\n",
       "      <td>0.414223</td>\n",
       "      <td>0.477188</td>\n",
       "      <td>0.134107</td>\n",
       "      <td>0.421787</td>\n",
       "      <td>0.892589</td>\n",
       "      <td>1.476419</td>\n",
       "      <td>1.944990</td>\n",
       "      <td>0.241409</td>\n",
       "      <td>21.004804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.018468</td>\n",
       "      <td>0.266052</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.088442</td>\n",
       "      <td>0.535803</td>\n",
       "      <td>0.018882</td>\n",
       "      <td>0.020040</td>\n",
       "      <td>1.443214</td>\n",
       "      <td>0.009287</td>\n",
       "      <td>0.178794</td>\n",
       "      <td>...</td>\n",
       "      <td>1.239302</td>\n",
       "      <td>0.379203</td>\n",
       "      <td>0.442348</td>\n",
       "      <td>0.121607</td>\n",
       "      <td>0.393708</td>\n",
       "      <td>0.836771</td>\n",
       "      <td>1.357219</td>\n",
       "      <td>1.793928</td>\n",
       "      <td>0.241458</td>\n",
       "      <td>19.439839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.018353</td>\n",
       "      <td>0.236522</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.079399</td>\n",
       "      <td>0.438120</td>\n",
       "      <td>0.013203</td>\n",
       "      <td>0.016620</td>\n",
       "      <td>1.201683</td>\n",
       "      <td>0.006795</td>\n",
       "      <td>0.149416</td>\n",
       "      <td>...</td>\n",
       "      <td>1.026139</td>\n",
       "      <td>0.314855</td>\n",
       "      <td>0.421064</td>\n",
       "      <td>0.106670</td>\n",
       "      <td>0.352381</td>\n",
       "      <td>0.755024</td>\n",
       "      <td>1.104780</td>\n",
       "      <td>1.443657</td>\n",
       "      <td>0.206547</td>\n",
       "      <td>16.853532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>0</th>\n",
       "      <td>0.017700</td>\n",
       "      <td>0.274786</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.091593</td>\n",
       "      <td>0.503290</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.022940</td>\n",
       "      <td>1.380106</td>\n",
       "      <td>0.008262</td>\n",
       "      <td>0.178308</td>\n",
       "      <td>...</td>\n",
       "      <td>1.179243</td>\n",
       "      <td>0.359990</td>\n",
       "      <td>0.475349</td>\n",
       "      <td>0.122300</td>\n",
       "      <td>0.395501</td>\n",
       "      <td>0.869601</td>\n",
       "      <td>1.254781</td>\n",
       "      <td>1.646822</td>\n",
       "      <td>0.246688</td>\n",
       "      <td>18.816239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2018-12-30</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">23</th>\n",
       "      <th>0</th>\n",
       "      <td>0.022957</td>\n",
       "      <td>0.180919</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>0.165186</td>\n",
       "      <td>1.386548</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>0.039228</td>\n",
       "      <td>3.153349</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>0.347246</td>\n",
       "      <td>...</td>\n",
       "      <td>3.166451</td>\n",
       "      <td>0.863148</td>\n",
       "      <td>1.005159</td>\n",
       "      <td>0.220152</td>\n",
       "      <td>0.734949</td>\n",
       "      <td>1.709667</td>\n",
       "      <td>2.997537</td>\n",
       "      <td>4.113963</td>\n",
       "      <td>0.262693</td>\n",
       "      <td>35.458590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.022963</td>\n",
       "      <td>0.217225</td>\n",
       "      <td>0.000437</td>\n",
       "      <td>0.178056</td>\n",
       "      <td>1.518671</td>\n",
       "      <td>0.011552</td>\n",
       "      <td>0.045846</td>\n",
       "      <td>3.454453</td>\n",
       "      <td>0.018654</td>\n",
       "      <td>0.382558</td>\n",
       "      <td>...</td>\n",
       "      <td>3.468031</td>\n",
       "      <td>0.945388</td>\n",
       "      <td>1.060805</td>\n",
       "      <td>0.239188</td>\n",
       "      <td>0.791983</td>\n",
       "      <td>1.903305</td>\n",
       "      <td>3.302366</td>\n",
       "      <td>4.532938</td>\n",
       "      <td>0.309564</td>\n",
       "      <td>38.698505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.022407</td>\n",
       "      <td>0.219712</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.180470</td>\n",
       "      <td>1.491726</td>\n",
       "      <td>0.012110</td>\n",
       "      <td>0.045439</td>\n",
       "      <td>3.419003</td>\n",
       "      <td>0.018429</td>\n",
       "      <td>0.384830</td>\n",
       "      <td>...</td>\n",
       "      <td>3.408581</td>\n",
       "      <td>0.933979</td>\n",
       "      <td>1.071634</td>\n",
       "      <td>0.241237</td>\n",
       "      <td>0.793314</td>\n",
       "      <td>1.860946</td>\n",
       "      <td>3.245837</td>\n",
       "      <td>4.451902</td>\n",
       "      <td>0.307307</td>\n",
       "      <td>38.403394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.022211</td>\n",
       "      <td>0.208126</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.180145</td>\n",
       "      <td>1.465121</td>\n",
       "      <td>0.010822</td>\n",
       "      <td>0.043686</td>\n",
       "      <td>3.363481</td>\n",
       "      <td>0.017804</td>\n",
       "      <td>0.380367</td>\n",
       "      <td>...</td>\n",
       "      <td>3.349450</td>\n",
       "      <td>0.919150</td>\n",
       "      <td>1.076603</td>\n",
       "      <td>0.239807</td>\n",
       "      <td>0.787455</td>\n",
       "      <td>1.816628</td>\n",
       "      <td>3.179967</td>\n",
       "      <td>4.360025</td>\n",
       "      <td>0.293550</td>\n",
       "      <td>37.830441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.020992</td>\n",
       "      <td>0.174021</td>\n",
       "      <td>0.000367</td>\n",
       "      <td>0.179834</td>\n",
       "      <td>1.289342</td>\n",
       "      <td>0.009332</td>\n",
       "      <td>0.038769</td>\n",
       "      <td>3.041228</td>\n",
       "      <td>0.015440</td>\n",
       "      <td>0.363511</td>\n",
       "      <td>...</td>\n",
       "      <td>2.948059</td>\n",
       "      <td>0.820913</td>\n",
       "      <td>1.097184</td>\n",
       "      <td>0.231222</td>\n",
       "      <td>0.753439</td>\n",
       "      <td>1.485539</td>\n",
       "      <td>2.752647</td>\n",
       "      <td>3.777555</td>\n",
       "      <td>0.238409</td>\n",
       "      <td>34.103171</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34921 rows × 265 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0_mean    1_mean    2_mean    3_mean    4_mean  \\\n",
       "Date       Hour Min                                                     \n",
       "2018-01-01 6    0    0.017662  0.251026 -0.000107  0.101448  0.586587   \n",
       "                15   0.017839  0.262316 -0.000002  0.096212  0.578203   \n",
       "                30   0.018468  0.266052  0.000081  0.088442  0.535803   \n",
       "                45   0.018353  0.236522  0.000302  0.079399  0.438120   \n",
       "           7    0    0.017700  0.274786  0.000275  0.091593  0.503290   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2018-12-30 23   0    0.022957  0.180919  0.000485  0.165186  1.386548   \n",
       "                15   0.022963  0.217225  0.000437  0.178056  1.518671   \n",
       "                30   0.022407  0.219712  0.000402  0.180470  1.491726   \n",
       "                45   0.022211  0.208126  0.000415  0.180145  1.465121   \n",
       "2018-12-31 0    0    0.020992  0.174021  0.000367  0.179834  1.289342   \n",
       "\n",
       "                       5_mean    6_mean    7_mean    8_mean    9_mean  ...  \\\n",
       "Date       Hour Min                                                    ...   \n",
       "2018-01-01 6    0    0.019011  0.016041  1.587028  0.009788  0.202507  ...   \n",
       "                15   0.018456  0.017548  1.548158  0.009548  0.193304  ...   \n",
       "                30   0.018882  0.020040  1.443214  0.009287  0.178794  ...   \n",
       "                45   0.013203  0.016620  1.201683  0.006795  0.149416  ...   \n",
       "           7    0    0.016807  0.022940  1.380106  0.008262  0.178308  ...   \n",
       "...                       ...       ...       ...       ...       ...  ...   \n",
       "2018-12-30 23   0    0.007888  0.039228  3.153349  0.016357  0.347246  ...   \n",
       "                15   0.011552  0.045846  3.454453  0.018654  0.382558  ...   \n",
       "                30   0.012110  0.045439  3.419003  0.018429  0.384830  ...   \n",
       "                45   0.010822  0.043686  3.363481  0.017804  0.380367  ...   \n",
       "2018-12-31 0    0    0.009332  0.038769  3.041228  0.015440  0.363511  ...   \n",
       "\n",
       "                     256_mean  257_mean  258_mean  259_mean  260_mean  \\\n",
       "Date       Hour Min                                                     \n",
       "2018-01-01 6    0    1.355020  0.423774  0.507407  0.140172  0.436238   \n",
       "                15   1.340479  0.414223  0.477188  0.134107  0.421787   \n",
       "                30   1.239302  0.379203  0.442348  0.121607  0.393708   \n",
       "                45   1.026139  0.314855  0.421064  0.106670  0.352381   \n",
       "           7    0    1.179243  0.359990  0.475349  0.122300  0.395501   \n",
       "...                       ...       ...       ...       ...       ...   \n",
       "2018-12-30 23   0    3.166451  0.863148  1.005159  0.220152  0.734949   \n",
       "                15   3.468031  0.945388  1.060805  0.239188  0.791983   \n",
       "                30   3.408581  0.933979  1.071634  0.241237  0.793314   \n",
       "                45   3.349450  0.919150  1.076603  0.239807  0.787455   \n",
       "2018-12-31 0    0    2.948059  0.820913  1.097184  0.231222  0.753439   \n",
       "\n",
       "                     261_mean  262_mean  263_mean  264_mean   265_mean  \n",
       "Date       Hour Min                                                     \n",
       "2018-01-01 6    0    0.837516  1.493704  1.971721  0.229733  21.341498  \n",
       "                15   0.892589  1.476419  1.944990  0.241409  21.004804  \n",
       "                30   0.836771  1.357219  1.793928  0.241458  19.439839  \n",
       "                45   0.755024  1.104780  1.443657  0.206547  16.853532  \n",
       "           7    0    0.869601  1.254781  1.646822  0.246688  18.816239  \n",
       "...                       ...       ...       ...       ...        ...  \n",
       "2018-12-30 23   0    1.709667  2.997537  4.113963  0.262693  35.458590  \n",
       "                15   1.903305  3.302366  4.532938  0.309564  38.698505  \n",
       "                30   1.860946  3.245837  4.451902  0.307307  38.403394  \n",
       "                45   1.816628  3.179967  4.360025  0.293550  37.830441  \n",
       "2018-12-31 0    0    1.485539  2.752647  3.777555  0.238409  34.103171  \n",
       "\n",
       "[34921 rows x 265 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "networkPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6850171408413117"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictedValueFromRF = inverse_pca(PCAPredictedDF.values,pca)\n",
    "predictedValueFromRF = inverse_standardize(predictedValueFromRF, scaler)\n",
    "r2_score(edge_data.iloc[maxlag:], predictedValueFromRF, multioutput='variance_weighted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/prediction/LGAPCA5Std30Min24lag.csv'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/prediction/%sPCA'%hub+str(pca_comps)+'Std'+str(granularity)+'Min'+str(maxlag)+'lag.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
